{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated WorldCover Download for Multiple Cities\n",
    "## Download and clip WorldCover 2021 data for all cities\n",
    "\n",
    "This notebook automatically:\n",
    "1. Finds all cities with Multi-Month stacks\n",
    "2. Loads their GeoJSON AOIs\n",
    "3. Determines the correct WorldCover tile\n",
    "4. Downloads and clips the data for each city"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import requests\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "from rasterio.merge import merge\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base paths\n",
    "BASE_PATH = \"/Users/timgotschim/Documents/LLM/infrared.city\"\n",
    "STACKS_FOLDER = os.path.join(BASE_PATH, \"21 Stacks\")\n",
    "GEOJSON_FOLDER = os.path.join(BASE_PATH, \"sentinel_data\")\n",
    "WORLDCOVER_FOLDER = os.path.join(BASE_PATH, \"worldcover\")\n",
    "\n",
    "# Create WorldCover folder\n",
    "os.makedirs(WORLDCOVER_FOLDER, exist_ok=True)\n",
    "\n",
    "# WorldCover base URL\n",
    "WORLDCOVER_BASE_URL = \"https://esa-worldcover.s3.eu-central-1.amazonaws.com/v200/2021/map/\"\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Stacks folder: {STACKS_FOLDER}\")\n",
    "print(f\"  GeoJSON folder: {GEOJSON_FOLDER}\")\n",
    "print(f\"  WorldCover output: {WORLDCOVER_FOLDER}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def get_worldcover_tile_name(lon, lat):\n    \"\"\"\n    Get WorldCover tile name from coordinates.\n    Tiles are 3x3 degrees, named like: ESA_WorldCover_10m_2021_v200_N51W003_Map.tif\n    \n    Tile naming rules:\n    - Latitude: N/S + 2 digits (rounded down to nearest 3)\n    - Longitude: E/W + 3 digits (rounded down to nearest 3)\n    - Tiles are named by their SW corner\n    \"\"\"\n    import math\n    \n    # Round down to nearest 3-degree tile boundary\n    # For latitude: N51 covers 51-54°N, S03 covers 3-6°S\n    lat_floor = math.floor(lat / 3) * 3\n    lat_dir = 'N' if lat_floor >= 0 else 'S'\n    lat_band = abs(lat_floor)\n    \n    # For longitude: E000 covers 0-3°E, W003 covers 3-6°W\n    lon_floor = math.floor(lon / 3) * 3\n    lon_dir = 'E' if lon_floor >= 0 else 'W'\n    lon_band = abs(lon_floor)\n    \n    tile_name = f\"ESA_WorldCover_10m_2021_v200_{lat_dir}{lat_band:02d}{lon_dir}{lon_band:03d}_Map.tif\"\n    return tile_name\n\n\ndef download_worldcover_tile(tile_name, output_dir):\n    \"\"\"\n    Download a WorldCover tile if it doesn't exist.\n    \"\"\"\n    output_path = os.path.join(output_dir, tile_name)\n    \n    if os.path.exists(output_path):\n        print(f\"    ✓ Tile already exists: {tile_name}\")\n        return output_path\n    \n    url = f\"{WORLDCOVER_BASE_URL}{tile_name}\"\n    print(f\"    Downloading: {tile_name}\")\n    print(f\"    URL: {url}\")\n    \n    try:\n        response = requests.get(url, stream=True)\n        response.raise_for_status()\n        \n        total_size = int(response.headers.get('content-length', 0))\n        \n        with open(output_path, 'wb') as f:\n            with tqdm(total=total_size, unit='B', unit_scale=True, desc=f\"    {tile_name}\") as pbar:\n                for chunk in response.iter_content(chunk_size=8192):\n                    if chunk:\n                        f.write(chunk)\n                        pbar.update(len(chunk))\n        \n        print(f\"    ✓ Downloaded: {tile_name}\")\n        return output_path\n        \n    except requests.exceptions.RequestException as e:\n        print(f\"    ✗ Error downloading {tile_name}: {e}\")\n        if os.path.exists(output_path):\n            os.remove(output_path)\n        return None\n\n\ndef clip_worldcover_to_aoi(worldcover_file, aoi_file, output_file):\n    \"\"\"\n    Clip WorldCover tile to AOI.\n    \"\"\"\n    # Load AOI\n    aoi = gpd.read_file(aoi_file)\n    if aoi.crs is None:\n        aoi.set_crs(\"EPSG:4326\", inplace=True)\n    aoi = aoi.to_crs(\"EPSG:4326\")\n    \n    geometries = [feature[\"geometry\"] for feature in aoi.__geo_interface__[\"features\"]]\n    \n    # Clip\n    with rasterio.open(worldcover_file) as src:\n        out_image, out_transform = mask(src, geometries, crop=True)\n        out_meta = src.meta.copy()\n        \n        # Update metadata\n        out_meta.update({\n            \"driver\": \"GTiff\",\n            \"height\": out_image.shape[1],\n            \"width\": out_image.shape[2],\n            \"transform\": out_transform,\n            \"compress\": \"lzw\"\n        })\n        \n        # Save clipped result\n        with rasterio.open(output_file, \"w\", **out_meta) as dest:\n            dest.write(out_image)\n    \n    return output_file\n\n\nprint(\"✓ Helper functions defined\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Discover Cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"DISCOVERING CITIES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Find all Multi-Month stack files\n",
    "stack_files = glob.glob(os.path.join(STACKS_FOLDER, \"*_MultiMonth_stack.tif\"))\n",
    "\n",
    "print(f\"\\nFound {len(stack_files)} Multi-Month stacks\")\n",
    "\n",
    "cities_to_process = []\n",
    "\n",
    "for stack_file in sorted(stack_files):\n",
    "    # Extract city name\n",
    "    filename = os.path.basename(stack_file)\n",
    "    city_name = filename.replace(\"_MultiMonth_stack.tif\", \"\")\n",
    "    \n",
    "    # Find GeoJSON\n",
    "    geojson_patterns = [\n",
    "        os.path.join(GEOJSON_FOLDER, f\"{city_name}.geojson\"),\n",
    "        os.path.join(GEOJSON_FOLDER, city_name, f\"{city_name}.geojson\"),\n",
    "    ]\n",
    "    \n",
    "    geojson_file = None\n",
    "    for pattern in geojson_patterns:\n",
    "        if os.path.exists(pattern):\n",
    "            geojson_file = pattern\n",
    "            break\n",
    "    \n",
    "    if not geojson_file:\n",
    "        print(f\"  ✗ {city_name}: No GeoJSON found\")\n",
    "        continue\n",
    "    \n",
    "    # Check if WorldCover already exists\n",
    "    worldcover_output = os.path.join(WORLDCOVER_FOLDER, f\"{city_name}_WorldCover_2021.tif\")\n",
    "    already_exists = os.path.exists(worldcover_output)\n",
    "    \n",
    "    cities_to_process.append({\n",
    "        \"name\": city_name,\n",
    "        \"stack_file\": stack_file,\n",
    "        \"geojson_file\": geojson_file,\n",
    "        \"worldcover_output\": worldcover_output,\n",
    "        \"already_exists\": already_exists\n",
    "    })\n",
    "    \n",
    "    status = \"✓ (exists)\" if already_exists else \"⚠ (needs download)\"\n",
    "    print(f\"  {city_name:20s} - GeoJSON: ✓  WorldCover: {status}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Cities ready for processing: {len(cities_to_process)}\")\n",
    "cities_needing_download = [c for c in cities_to_process if not c['already_exists']]\n",
    "print(f\"Cities needing WorldCover download: {len(cities_needing_download)}\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Download and Clip WorldCover for Each City"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DOWNLOADING AND PROCESSING WORLDCOVER\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "successful_downloads = 0\n",
    "failed_downloads = 0\n",
    "skipped_existing = 0\n",
    "\n",
    "for city_data in cities_to_process:\n",
    "    city_name = city_data[\"name\"]\n",
    "    geojson_file = city_data[\"geojson_file\"]\n",
    "    worldcover_output = city_data[\"worldcover_output\"]\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Processing: {city_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Skip if already exists\n",
    "    if city_data[\"already_exists\"]:\n",
    "        print(f\"  ✓ WorldCover already exists, skipping download\")\n",
    "        skipped_existing += 1\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        # Load AOI to get center coordinates\n",
    "        aoi = gpd.read_file(geojson_file)\n",
    "        bounds = aoi.total_bounds  # [minx, miny, maxx, maxy]\n",
    "        center_lon = (bounds[0] + bounds[2]) / 2\n",
    "        center_lat = (bounds[1] + bounds[3]) / 2\n",
    "        \n",
    "        print(f\"  City center: {center_lat:.4f}°N, {center_lon:.4f}°E\")\n",
    "        \n",
    "        # Determine WorldCover tile\n",
    "        tile_name = get_worldcover_tile_name(center_lon, center_lat)\n",
    "        print(f\"  Required tile: {tile_name}\")\n",
    "        \n",
    "        # Download tile\n",
    "        tile_path = download_worldcover_tile(tile_name, WORLDCOVER_FOLDER)\n",
    "        \n",
    "        if not tile_path:\n",
    "            print(f\"  ✗ Failed to download WorldCover tile\")\n",
    "            failed_downloads += 1\n",
    "            continue\n",
    "        \n",
    "        # Clip to AOI\n",
    "        print(f\"  Clipping to AOI...\")\n",
    "        clip_worldcover_to_aoi(tile_path, geojson_file, worldcover_output)\n",
    "        \n",
    "        # Verify output\n",
    "        with rasterio.open(worldcover_output) as src:\n",
    "            data = src.read(1)\n",
    "            unique, counts = np.unique(data, return_counts=True)\n",
    "            \n",
    "            print(f\"  ✓ Clipped WorldCover saved: {worldcover_output}\")\n",
    "            print(f\"    Dimensions: {src.height}x{src.width} pixels\")\n",
    "            print(f\"    File size: {os.path.getsize(worldcover_output) / (1024**2):.2f} MB\")\n",
    "            \n",
    "            # Calculate green coverage\n",
    "            green_classes = [10, 20, 30, 95]\n",
    "            green_mask = np.isin(data, green_classes)\n",
    "            green_percentage = 100 * green_mask.sum() / data.size\n",
    "            print(f\"    Green coverage: {green_percentage:.2f}%\")\n",
    "        \n",
    "        successful_downloads += 1\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error processing {city_name}: {e}\")\n",
    "        failed_downloads += 1\n",
    "        continue\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PROCESSING SUMMARY\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"  Total cities: {len(cities_to_process)}\")\n",
    "print(f\"  Already existed: {skipped_existing}\")\n",
    "print(f\"  Successfully downloaded: {successful_downloads}\")\n",
    "print(f\"  Failed: {failed_downloads}\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Verify All Cities Have WorldCover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL VERIFICATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "complete_cities = 0\n",
    "incomplete_cities = []\n",
    "\n",
    "print(\"\\nCity Status:\")\n",
    "for city_data in cities_to_process:\n",
    "    city_name = city_data[\"name\"]\n",
    "    has_stack = os.path.exists(city_data[\"stack_file\"])\n",
    "    has_geojson = os.path.exists(city_data[\"geojson_file\"])\n",
    "    has_worldcover = os.path.exists(city_data[\"worldcover_output\"])\n",
    "    \n",
    "    if has_stack and has_geojson and has_worldcover:\n",
    "        print(f\"  ✓ {city_name:20s} - Complete (Stack, GeoJSON, WorldCover)\")\n",
    "        complete_cities += 1\n",
    "    else:\n",
    "        status_parts = []\n",
    "        if not has_stack:\n",
    "            status_parts.append(\"missing Stack\")\n",
    "        if not has_geojson:\n",
    "            status_parts.append(\"missing GeoJSON\")\n",
    "        if not has_worldcover:\n",
    "            status_parts.append(\"missing WorldCover\")\n",
    "        \n",
    "        status = \", \".join(status_parts)\n",
    "        print(f\"  ✗ {city_name:20s} - Incomplete ({status})\")\n",
    "        incomplete_cities.append(city_name)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Cities ready for training: {complete_cities}/{len(cities_to_process)}\")\n",
    "\n",
    "if incomplete_cities:\n",
    "    print(f\"\\nIncomplete cities: {', '.join(incomplete_cities)}\")\n",
    "    print(\"\\nPlease check the errors above and ensure:\")\n",
    "    print(\"  1. GeoJSON files exist in the sentinel_data folder\")\n",
    "    print(\"  2. Internet connection is stable for downloads\")\n",
    "    print(\"  3. Sufficient disk space is available\")\n",
    "else:\n",
    "    print(\"\\n✓ All cities are complete and ready for training!\")\n",
    "    print(\"\\nYou can now run the Multi_City_WorldCover_Training notebook.\")\n",
    "\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. WorldCover Class Legend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"WORLDCOVER 2021 CLASS LEGEND\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nClass | Description              | Used for Training\")\n",
    "print(\"-\" * 80)\n",
    "print(\"  10  | Tree cover               | ✓ GREEN\")\n",
    "print(\"  20  | Shrubland                | ✓ GREEN\")\n",
    "print(\"  30  | Grassland                | ✓ GREEN\")\n",
    "print(\"  40  | Cropland                 | ✗ Non-green\")\n",
    "print(\"  50  | Built-up                 | ✗ Non-green\")\n",
    "print(\"  60  | Bare/sparse vegetation   | ✗ Non-green\")\n",
    "print(\"  70  | Snow and ice             | ✗ Non-green\")\n",
    "print(\"  80  | Permanent water bodies   | ✗ Non-green\")\n",
    "print(\"  90  | Herbaceous wetland       | ✗ Non-green\")\n",
    "print(\"  95  | Mangroves                | ✓ GREEN\")\n",
    "print(\" 100  | Moss and lichen          | ✗ Non-green\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Green classes used for training: 10, 20, 30, 95\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}