{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-City Green Space Detection\n",
    "## Training Random Forest with WorldCover 2021 as Ground Truth\n",
    "\n",
    "**Training Cities:** 9 cities for robust model training\n",
    "\n",
    "**Key Features:**\n",
    "- Uses **WorldCover 2021** as ground truth for training\n",
    "- Green classes: Tree cover (10), Shrubland (20), Grassland (30), Mangroves (95)\n",
    "- Multi-temporal Sentinel-2 data (April, August, November)\n",
    "- 21 bands: 4 spectral bands × 3 months + 3 vegetation indices × 3 months\n",
    "- **Cross-city training** for better generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio.warp import reproject, Resampling\n",
    "from pathlib import Path\n",
    "import geopandas as gpd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base paths\n",
    "BASE_PATH = \"/Users/timgotschim/Documents/LLM/infrared.city\"\n",
    "STACKS_FOLDER = os.path.join(BASE_PATH, \"21 Stacks\")\n",
    "GEOJSON_FOLDER = os.path.join(BASE_PATH, \"sentinel_data\")\n",
    "WORLDCOVER_FOLDER = os.path.join(BASE_PATH, \"worldcover\")\n",
    "\n",
    "# Output folder\n",
    "OUTPUT_FOLDER = os.path.join(BASE_PATH, \"multi_city_training\")\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "# Create timestamped run folder\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "RUN_FOLDER = os.path.join(OUTPUT_FOLDER, f\"run_{timestamp}\")\n",
    "os.makedirs(RUN_FOLDER, exist_ok=True)\n",
    "\n",
    "# WorldCover green classes\n",
    "GREEN_CLASSES = [10, 20, 30, 95]  # Tree, Shrub, Grass, Mangroves\n",
    "\n",
    "print(\"✓ Configuration loaded\")\n",
    "print(f\"  Stacks folder: {STACKS_FOLDER}\")\n",
    "print(f\"  GeoJSON folder: {GEOJSON_FOLDER}\")\n",
    "print(f\"  WorldCover folder: {WORLDCOVER_FOLDER}\")\n",
    "print(f\"  Output folder: {RUN_FOLDER}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Discover Available Cities\n",
    "### Find all cities with Multi-Month stacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"DISCOVERING AVAILABLE CITIES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Find all Multi-Month stack files\n",
    "stack_files = glob.glob(os.path.join(STACKS_FOLDER, \"*_MultiMonth_stack.tif\"))\n",
    "\n",
    "print(f\"\\nFound {len(stack_files)} Multi-Month stacks:\")\n",
    "\n",
    "cities_data = []\n",
    "\n",
    "for stack_file in sorted(stack_files):\n",
    "    # Extract city name from filename\n",
    "    filename = os.path.basename(stack_file)\n",
    "    city_name = filename.replace(\"_MultiMonth_stack.tif\", \"\")\n",
    "    \n",
    "    # Try to find corresponding GeoJSON\n",
    "    geojson_patterns = [\n",
    "        os.path.join(GEOJSON_FOLDER, f\"{city_name}.geojson\"),\n",
    "        os.path.join(GEOJSON_FOLDER, city_name, f\"{city_name}.geojson\"),\n",
    "        os.path.join(GEOJSON_FOLDER, f\"{city_name.lower()}.geojson\"),\n",
    "    ]\n",
    "    \n",
    "    geojson_file = None\n",
    "    for pattern in geojson_patterns:\n",
    "        if os.path.exists(pattern):\n",
    "            geojson_file = pattern\n",
    "            break\n",
    "    \n",
    "    # Try to find WorldCover file\n",
    "    worldcover_patterns = [\n",
    "        os.path.join(WORLDCOVER_FOLDER, f\"{city_name}_WorldCover_2021.tif\"),\n",
    "        os.path.join(WORLDCOVER_FOLDER, f\"{city_name}_WorldCover.tif\"),\n",
    "        os.path.join(WORLDCOVER_FOLDER, city_name, f\"{city_name}_WorldCover_2021.tif\"),\n",
    "    ]\n",
    "    \n",
    "    worldcover_file = None\n",
    "    for pattern in worldcover_patterns:\n",
    "        if os.path.exists(pattern):\n",
    "            worldcover_file = pattern\n",
    "            break\n",
    "    \n",
    "    cities_data.append({\n",
    "        \"name\": city_name,\n",
    "        \"stack_file\": stack_file,\n",
    "        \"geojson_file\": geojson_file,\n",
    "        \"worldcover_file\": worldcover_file\n",
    "    })\n",
    "    \n",
    "    status_geojson = \"✓\" if geojson_file else \"✗\"\n",
    "    status_worldcover = \"✓\" if worldcover_file else \"✗\"\n",
    "    \n",
    "    print(f\"  {city_name:20s} - Stack: ✓  GeoJSON: {status_geojson}  WorldCover: {status_worldcover}\")\n",
    "\n",
    "# Filter cities with all required data\n",
    "complete_cities = [\n",
    "    city for city in cities_data \n",
    "    if city[\"geojson_file\"] and city[\"worldcover_file\"]\n",
    "]\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Cities with complete data: {len(complete_cities)}\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "if len(complete_cities) == 0:\n",
    "    raise ValueError(\"No cities with complete data found! Check your file paths.\")\n",
    "\n",
    "for city in complete_cities:\n",
    "    print(f\"  ✓ {city['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load and Process All Cities\n",
    "### Load Sentinel-2 stacks and create WorldCover labels for each city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*70)\nprint(\"LOADING AND PROCESSING ALL CITIES\")\nprint(\"=\"*70)\n\n# Expected number of bands (set to None to auto-detect from first city)\nEXPECTED_BANDS = None\n\nall_X = []  # Features from all cities\nall_y = []  # Labels from all cities\ncity_info = []  # Track which city each sample came from\nskipped_cities = []  # Track skipped cities\n\nfor city_data in tqdm(complete_cities, desc=\"Processing cities\"):\n    city_name = city_data[\"name\"]\n    stack_file = city_data[\"stack_file\"]\n    worldcover_file = city_data[\"worldcover_file\"]\n    \n    print(f\"\\n{'='*70}\")\n    print(f\"Processing: {city_name}\")\n    print(f\"{'='*70}\")\n    \n    try:\n        # Load Sentinel-2 stack\n        with rasterio.open(stack_file) as src:\n            X_stack = src.read()  # Shape: (n_bands, height, width)\n            stack_transform = src.transform\n            stack_shape = (src.height, src.width)\n            stack_crs = src.crs\n        \n        n_bands = X_stack.shape[0]\n        print(f\"  ✓ Loaded Sentinel-2 stack: {X_stack.shape} ({n_bands} bands)\")\n        \n        # Set expected bands from first city, or check consistency\n        if EXPECTED_BANDS is None:\n            EXPECTED_BANDS = n_bands\n            print(f\"  ℹ Setting expected bands to {EXPECTED_BANDS}\")\n        elif n_bands != EXPECTED_BANDS:\n            print(f\"  ⚠ SKIPPING: Expected {EXPECTED_BANDS} bands, but found {n_bands} bands\")\n            skipped_cities.append({\"name\": city_name, \"reason\": f\"Band mismatch: {n_bands} vs {EXPECTED_BANDS}\"})\n            continue\n        \n        # Load and reproject WorldCover to match Sentinel-2\n        with rasterio.open(worldcover_file) as src:\n            worldcover_data = np.empty(stack_shape, dtype=np.uint8)\n            \n            reproject(\n                source=rasterio.band(src, 1),\n                destination=worldcover_data,\n                src_transform=src.transform,\n                src_crs=src.crs,\n                dst_transform=stack_transform,\n                dst_crs=stack_crs,\n                resampling=Resampling.nearest\n            )\n        \n        # Convert to binary green/non-green labels\n        labels = np.isin(worldcover_data, GREEN_CLASSES).astype(np.uint8)\n        \n        green_percentage = 100 * labels.sum() / labels.size\n        print(f\"  ✓ WorldCover labels: {labels.shape} ({green_percentage:.2f}% green)\")\n        \n        # Reshape for sklearn: (n_samples, n_features)\n        n_pixels = X_stack.shape[1] * X_stack.shape[2]\n        \n        X = X_stack.reshape(n_bands, -1).T  # Shape: (n_pixels, n_bands)\n        y = labels.flatten()  # Shape: (n_pixels,)\n        \n        # Remove NaN values\n        valid_mask = ~np.isnan(X).any(axis=1)\n        X_clean = X[valid_mask]\n        y_clean = y[valid_mask]\n        \n        print(f\"  ✓ Valid samples: {len(X_clean):,} ({100*len(X_clean)/n_pixels:.1f}% of pixels)\")\n        print(f\"    - Green: {np.sum(y_clean == 1):,} ({100*np.sum(y_clean == 1)/len(y_clean):.2f}%)\")\n        print(f\"    - Non-green: {np.sum(y_clean == 0):,} ({100*np.sum(y_clean == 0)/len(y_clean):.2f}%)\")\n        \n        # Add to combined dataset\n        all_X.append(X_clean)\n        all_y.append(y_clean)\n        city_info.extend([city_name] * len(X_clean))\n        \n    except Exception as e:\n        print(f\"  ✗ Error processing {city_name}: {e}\")\n        skipped_cities.append({\"name\": city_name, \"reason\": str(e)})\n        continue\n\nprint(f\"\\n{'='*70}\")\nprint(\"DATA AGGREGATION\")\nprint(f\"{'='*70}\")\n\n# Report skipped cities\nif skipped_cities:\n    print(f\"\\n⚠ Skipped {len(skipped_cities)} cities due to issues:\")\n    for city in skipped_cities:\n        print(f\"  - {city['name']}: {city['reason']}\")\n\n# Check if we have any data\nif len(all_X) == 0:\n    raise ValueError(\"No valid city data loaded! Check that all stacks have the same number of bands.\")\n\n# Combine all data\nX_combined = np.vstack(all_X)\ny_combined = np.hstack(all_y)\ncity_info = np.array(city_info)\n\nprint(f\"\\nCombined dataset:\")\nprint(f\"  Cities included: {len(all_X)}\")\nprint(f\"  Total samples: {len(X_combined):,}\")\nprint(f\"  Features (bands): {X_combined.shape[1]}\")\nprint(f\"  Green samples: {np.sum(y_combined == 1):,} ({100*np.sum(y_combined == 1)/len(y_combined):.2f}%)\")\nprint(f\"  Non-green samples: {np.sum(y_combined == 0):,} ({100*np.sum(y_combined == 0)/len(y_combined):.2f}%)\")\n\n# Save city distribution\nprint(f\"\\nSamples per city:\")\nunique_cities = np.unique(city_info)\nfor city_name in unique_cities:\n    city_samples = np.sum(city_info == city_name)\n    print(f\"  {city_name:20s}: {city_samples:>10,} samples\")\n\nprint(f\"{'='*70}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train-Test Split\n",
    "### Split data for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAIN-TEST SPLIT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Split data (80-20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_combined, y_combined, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y_combined\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset split:\")\n",
    "print(f\"  Training samples: {len(X_train):,}\")\n",
    "print(f\"    - Green: {np.sum(y_train == 1):,} ({100*np.sum(y_train == 1)/len(y_train):.2f}%)\")\n",
    "print(f\"    - Non-green: {np.sum(y_train == 0):,} ({100*np.sum(y_train == 0)/len(y_train):.2f}%)\")\n",
    "print(f\"\\n  Testing samples: {len(X_test):,}\")\n",
    "print(f\"    - Green: {np.sum(y_test == 1):,} ({100*np.sum(y_test == 1)/len(y_test):.2f}%)\")\n",
    "print(f\"    - Non-green: {np.sum(y_test == 0):,} ({100*np.sum(y_test == 0)/len(y_test):.2f}%)\")\n",
    "\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train Random Forest Model\n",
    "### Train on multi-city dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING RANDOM FOREST MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Initialize Random Forest\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=25,\n",
    "    min_samples_split=50,\n",
    "    min_samples_leaf=20,\n",
    "    max_features='sqrt',\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(f\"\\nRandom Forest parameters:\")\n",
    "print(f\"  n_estimators: {rf.n_estimators}\")\n",
    "print(f\"  max_depth: {rf.max_depth}\")\n",
    "print(f\"  min_samples_split: {rf.min_samples_split}\")\n",
    "print(f\"  min_samples_leaf: {rf.min_samples_leaf}\")\n",
    "print(f\"  max_features: {rf.max_features}\")\n",
    "\n",
    "print(f\"\\nTraining Random Forest...\")\n",
    "print(f\"  Training on {len(complete_cities)} cities\")\n",
    "print(f\"  Training samples: {len(X_train):,}\")\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\n✓ Model trained successfully\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "\n",
    "print(f\"\\nModel Performance (trained on {len(complete_cities)} cities):\")\n",
    "print(f\"  Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"  Precision: {precision:.4f}\")\n",
    "print(f\"  Recall:    {recall:.4f}\")\n",
    "print(f\"  F1-Score:  {f1:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"                 Predicted\")\n",
    "print(f\"               Non-Green  Green\")\n",
    "print(f\"Actual Non-Green  {cm[0,0]:>8,}  {cm[0,1]:>8,}\")\n",
    "print(f\"       Green      {cm[1,0]:>8,}  {cm[1,1]:>8,}\")\n",
    "\n",
    "# Save metrics\n",
    "metrics = {\n",
    "    \"model\": \"RandomForest\",\n",
    "    \"ground_truth\": \"WorldCover_2021\",\n",
    "    \"training_cities\": [city['name'] for city in complete_cities],\n",
    "    \"n_cities\": len(complete_cities),\n",
    "    \"total_training_samples\": int(len(X_train)),\n",
    "    \"total_testing_samples\": int(len(X_test)),\n",
    "    \"accuracy\": float(accuracy),\n",
    "    \"precision\": float(precision),\n",
    "    \"recall\": float(recall),\n",
    "    \"f1_score\": float(f1),\n",
    "    \"confusion_matrix\": cm.tolist()\n",
    "}\n",
    "\n",
    "with open(os.path.join(RUN_FOLDER, \"metrics.json\"), \"w\") as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "print(f\"\\n✓ Metrics saved to: {RUN_FOLDER}/metrics.json\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Non-Green', 'Green'],\n",
    "            yticklabels=['Non-Green', 'Green'],\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.title(f'Confusion Matrix - Multi-City Random Forest\\n(Trained on {len(complete_cities)} cities)', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(RUN_FOLDER, 'confusion_matrix.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Confusion matrix saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Get feature importances\nimportances = rf.feature_importances_\nn_features = len(importances)\n\n# Generate band names based on actual number of bands\nif n_features == 21:\n    # 21 bands: 4 spectral + 3 indices per month × 3 months\n    band_names = [\n        'B02-Apr', 'B03-Apr', 'B04-Apr', 'B08-Apr', 'NDVI-Apr', 'EVI-Apr', 'SAVI-Apr',\n        'B02-Aug', 'B03-Aug', 'B04-Aug', 'B08-Aug', 'NDVI-Aug', 'EVI-Aug', 'SAVI-Aug',\n        'B02-Nov', 'B03-Nov', 'B04-Nov', 'B08-Nov', 'NDVI-Nov', 'EVI-Nov', 'SAVI-Nov'\n    ]\nelif n_features == 12:\n    # 12 bands: 4 spectral per month × 3 months (no indices)\n    band_names = [\n        'B02-Apr', 'B03-Apr', 'B04-Apr', 'B08-Apr',\n        'B02-Aug', 'B03-Aug', 'B04-Aug', 'B08-Aug',\n        'B02-Nov', 'B03-Nov', 'B04-Nov', 'B08-Nov'\n    ]\nelif n_features == 14:\n    # 14 bands: possibly 4 spectral + some indices per month\n    band_names = [f'Band_{i+1}' for i in range(n_features)]\n    print(f\"ℹ Using generic band names for {n_features} bands\")\nelse:\n    # Generic fallback\n    band_names = [f'Band_{i+1}' for i in range(n_features)]\n    print(f\"ℹ Using generic band names for {n_features} bands\")\n\n# Sort by importance\nindices = np.argsort(importances)[::-1]\n\n# Plot feature importances\nplt.figure(figsize=(12, max(8, n_features * 0.4)))\nplt.barh(range(len(importances)), importances[indices], color='steelblue')\nplt.yticks(range(len(importances)), [band_names[i] for i in indices])\nplt.xlabel('Feature Importance', fontsize=12)\nplt.title(f'Random Forest Feature Importance\\n(Trained on {len(all_X)} cities, {n_features} bands)', \n          fontsize=14, fontweight='bold')\nplt.grid(axis='x', alpha=0.3)\nplt.tight_layout()\nplt.savefig(os.path.join(RUN_FOLDER, 'feature_importance.png'), dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(\"✓ Feature importance plot saved\")\nprint(f\"\\nTop 10 most important features:\")\nfor i in range(min(10, len(importances))):\n    idx = indices[i]\n    print(f\"  {i+1:2d}. {band_names[idx]:12s}: {importances[idx]:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the trained model\n",
    "model_file = os.path.join(RUN_FOLDER, 'random_forest_model.pkl')\n",
    "joblib.dump(rf, model_file)\n",
    "\n",
    "print(f\"✓ Model saved to: {model_file}\")\n",
    "print(f\"\\nTo load the model later:\")\n",
    "print(f\"  import joblib\")\n",
    "print(f\"  rf = joblib.load('{model_file}')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Per-City Performance Analysis (Optional)\n",
    "### Evaluate model performance on each city individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PER-CITY PERFORMANCE ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "per_city_results = []\n",
    "\n",
    "for city_data in complete_cities:\n",
    "    city_name = city_data[\"name\"]\n",
    "    stack_file = city_data[\"stack_file\"]\n",
    "    worldcover_file = city_data[\"worldcover_file\"]\n",
    "    \n",
    "    print(f\"\\n{city_name}:\")\n",
    "    \n",
    "    try:\n",
    "        # Load city data\n",
    "        with rasterio.open(stack_file) as src:\n",
    "            X_stack = src.read()\n",
    "            stack_transform = src.transform\n",
    "            stack_shape = (src.height, src.width)\n",
    "            stack_crs = src.crs\n",
    "        \n",
    "        with rasterio.open(worldcover_file) as src:\n",
    "            worldcover_data = np.empty(stack_shape, dtype=np.uint8)\n",
    "            reproject(\n",
    "                source=rasterio.band(src, 1),\n",
    "                destination=worldcover_data,\n",
    "                src_transform=src.transform,\n",
    "                src_crs=src.crs,\n",
    "                dst_transform=stack_transform,\n",
    "                dst_crs=stack_crs,\n",
    "                resampling=Resampling.nearest\n",
    "            )\n",
    "        \n",
    "        labels = np.isin(worldcover_data, GREEN_CLASSES).astype(np.uint8)\n",
    "        \n",
    "        # Reshape and clean\n",
    "        X = X_stack.reshape(X_stack.shape[0], -1).T\n",
    "        y = labels.flatten()\n",
    "        valid_mask = ~np.isnan(X).any(axis=1)\n",
    "        X_city = X[valid_mask]\n",
    "        y_city = y[valid_mask]\n",
    "        \n",
    "        # Predict\n",
    "        y_pred_city = rf.predict(X_city)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        acc = accuracy_score(y_city, y_pred_city)\n",
    "        prec = precision_score(y_city, y_pred_city, zero_division=0)\n",
    "        rec = recall_score(y_city, y_pred_city, zero_division=0)\n",
    "        f1_city = f1_score(y_city, y_pred_city, zero_division=0)\n",
    "        \n",
    "        print(f\"  Accuracy:  {acc:.4f}\")\n",
    "        print(f\"  Precision: {prec:.4f}\")\n",
    "        print(f\"  Recall:    {rec:.4f}\")\n",
    "        print(f\"  F1-Score:  {f1_city:.4f}\")\n",
    "        \n",
    "        per_city_results.append({\n",
    "            \"city\": city_name,\n",
    "            \"accuracy\": float(acc),\n",
    "            \"precision\": float(prec),\n",
    "            \"recall\": float(rec),\n",
    "            \"f1_score\": float(f1_city)\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {e}\")\n",
    "\n",
    "# Save per-city results\n",
    "with open(os.path.join(RUN_FOLDER, \"per_city_metrics.json\"), \"w\") as f:\n",
    "    json.dump(per_city_results, f, indent=2)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"✓ Per-city metrics saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MULTI-CITY TRAINING - SUMMARY REPORT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nGround Truth: WorldCover 2021\")\n",
    "print(f\"Green Classes: Tree cover (10), Shrubland (20), Grassland (30), Mangroves (95)\")\n",
    "\n",
    "print(f\"\\nTraining Data:\")\n",
    "print(f\"  Cities: {len(complete_cities)}\")\n",
    "for city in complete_cities:\n",
    "    print(f\"    - {city['name']}\")\n",
    "\n",
    "print(f\"\\n  Total training samples: {len(X_train):,}\")\n",
    "print(f\"  Total testing samples:  {len(X_test):,}\")\n",
    "\n",
    "print(f\"\\nModel Performance (Overall):\")\n",
    "print(f\"  Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"  Precision: {precision:.4f}\")\n",
    "print(f\"  Recall:    {recall:.4f}\")\n",
    "print(f\"  F1-Score:  {f1:.4f}\")\n",
    "\n",
    "if per_city_results:\n",
    "    print(f\"\\nPer-City Performance (Average):\")\n",
    "    avg_acc = np.mean([r['accuracy'] for r in per_city_results])\n",
    "    avg_prec = np.mean([r['precision'] for r in per_city_results])\n",
    "    avg_rec = np.mean([r['recall'] for r in per_city_results])\n",
    "    avg_f1 = np.mean([r['f1_score'] for r in per_city_results])\n",
    "    print(f\"  Accuracy:  {avg_acc:.4f}\")\n",
    "    print(f\"  Precision: {avg_prec:.4f}\")\n",
    "    print(f\"  Recall:    {avg_rec:.4f}\")\n",
    "    print(f\"  F1-Score:  {avg_f1:.4f}\")\n",
    "\n",
    "print(f\"\\nOutput Files:\")\n",
    "print(f\"  Results folder: {RUN_FOLDER}\")\n",
    "print(f\"  - metrics.json (overall performance)\")\n",
    "print(f\"  - per_city_metrics.json (individual city performance)\")\n",
    "print(f\"  - confusion_matrix.png\")\n",
    "print(f\"  - feature_importance.png\")\n",
    "print(f\"  - random_forest_model.pkl (trained model)\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(f\"✓ TRAINING COMPLETE!\")\n",
    "print(f\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}