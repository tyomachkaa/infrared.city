{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Multi-City Green Space Detection\n## Training Random Forest with WorldCover 2021 as Ground Truth\n\nTraining cities: 11 cities for robust model generalisation\n\n- Uses WorldCover 2021 as ground truth (green classes: tree cover, shrubland, grassland, mangroves)\n- Multi-temporal Sentinel-2 data (April, August, November)\n- 21 bands: 4 spectral + 3 vegetation indices per month\n- Cross-city training for better generalisation"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%pip install numpy rasterio geopandas scikit-learn matplotlib seaborn tqdm\n\nimport json\nimport os\nimport glob\nimport numpy as np\nimport rasterio\nfrom rasterio.warp import reproject, Resampling\nfrom pathlib import Path\nimport geopandas as gpd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(\"Libraries imported\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded\n",
      "  Project root: /Users/tyomachka/Desktop/WU/Data_Lab.TMP/rep.infrared.city\n",
      "  Data path: /Users/tyomachka/Desktop/WU/Data_Lab.TMP/rep.infrared.city/data\n",
      "  Models path: /Users/tyomachka/Desktop/WU/Data_Lab.TMP/rep.infrared.city/models\n",
      "  Output folder: /Users/tyomachka/Desktop/WU/Data_Lab.TMP/rep.infrared.city/outputs/multi_city_training/run_20260127_231351\n",
      "  Target cities: 11\n",
      "    - Amsterdam\n",
      "    - Auckland\n",
      "    - Barcelona\n",
      "    - Sydney\n",
      "    - Toronto\n",
      "    - Vienna\n",
      "    - London\n",
      "    - Melbourne\n",
      "    - Paris\n",
      "    - San_Francisco\n",
      "    - Seattle\n"
     ]
    }
   ],
   "source": [
    "# Base paths - using relative paths from project root\n",
    "# Run notebooks from the project root directory: python -m jupyter notebook\n",
    "import os\n",
    "\n",
    "# Find project root (go up from notebooks/training/)\n",
    "NOTEBOOK_DIR = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(NOTEBOOK_DIR, \"..\", \"..\"))\n",
    "\n",
    "# If running from project root directly\n",
    "if os.path.exists(\"data\") and os.path.exists(\"models\"):\n",
    "    PROJECT_ROOT = os.getcwd()\n",
    "# If running from notebooks/training/\n",
    "elif os.path.exists(\"../../data\") and os.path.exists(\"../../models\"):\n",
    "    PROJECT_ROOT = os.path.abspath(\"../..\")\n",
    "# Fallback\n",
    "else:\n",
    "    PROJECT_ROOT = os.getcwd()\n",
    "    print(f\"Warning: Could not detect project root. Using: {PROJECT_ROOT}\")\n",
    "\n",
    "# Derived paths\n",
    "DATA_PATH = os.path.join(PROJECT_ROOT, \"data\")\n",
    "MODELS_PATH = os.path.join(PROJECT_ROOT, \"models\")\n",
    "GEOJSON_FOLDER = os.path.join(DATA_PATH, \"aois\")\n",
    "\n",
    "# Output folder\n",
    "OUTPUT_FOLDER = os.path.join(PROJECT_ROOT, \"outputs\", \"multi_city_training\")\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "os.makedirs(MODELS_PATH, exist_ok=True)\n",
    "\n",
    "# Create timestamped run folder\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "RUN_FOLDER = os.path.join(OUTPUT_FOLDER, f\"run_{timestamp}\")\n",
    "os.makedirs(RUN_FOLDER, exist_ok=True)\n",
    "\n",
    "# WorldCover green classes\n",
    "GREEN_CLASSES = [10, 20, 30, 95]  # Tree, Shrub, Grass, Mangroves\n",
    "\n",
    "# Define cities with their specific file locations\n",
    "# All data now in unified data/ folder structure\n",
    "CITY_FILES = {\n",
    "    \"Amsterdam\": {\n",
    "        \"stack\": os.path.join(DATA_PATH, \"sentinel_stacks\", \"Amsterdam_MultiMonth_stack.tif\"),\n",
    "        \"worldcover\": os.path.join(DATA_PATH, \"worldcover\", \"Amsterdam_WorldCover_2021.tif\"),\n",
    "        \"geojson\": os.path.join(GEOJSON_FOLDER, \"Amsterdam.geojson\"),\n",
    "    },\n",
    "    \"Auckland\": {\n",
    "        \"stack\": os.path.join(DATA_PATH, \"sentinel_stacks\", \"Auckland_MultiMonth_stack.tif\"),\n",
    "        \"worldcover\": os.path.join(DATA_PATH, \"worldcover\", \"Auckland_WorldCover_2021.tif\"),\n",
    "        \"geojson\": os.path.join(GEOJSON_FOLDER, \"Auckland.geojson\"),\n",
    "    },\n",
    "    \"Barcelona\": {\n",
    "        \"stack\": os.path.join(DATA_PATH, \"sentinel_stacks\", \"Barcelona_MultiMonth_stack.tif\"),\n",
    "        \"worldcover\": os.path.join(DATA_PATH, \"worldcover\", \"Barcelona_WorldCover_2021.tif\"),\n",
    "        \"geojson\": os.path.join(GEOJSON_FOLDER, \"Barcelona.geojson\"),\n",
    "    },\n",
    "    \"Sydney\": {\n",
    "        \"stack\": os.path.join(DATA_PATH, \"sentinel_stacks\", \"Sydney_MultiMonth_stack.tif\"),\n",
    "        \"worldcover\": os.path.join(DATA_PATH, \"worldcover\", \"Sydney_WorldCover_2021.tif\"),\n",
    "        \"geojson\": os.path.join(GEOJSON_FOLDER, \"Sydney.geojson\"),\n",
    "    },\n",
    "    \"Toronto\": {\n",
    "        \"stack\": os.path.join(DATA_PATH, \"sentinel_stacks\", \"Toronto_MultiMonth_stack.tif\"),\n",
    "        \"worldcover\": os.path.join(DATA_PATH, \"worldcover\", \"Toronto_WorldCover_2021.tif\"),\n",
    "        \"geojson\": os.path.join(GEOJSON_FOLDER, \"Toronto.geojson\"),\n",
    "    },\n",
    "    \"Vienna\": {\n",
    "        \"stack\": os.path.join(DATA_PATH, \"sentinel_stacks\", \"Wien_MultiMonth_stack.tif\"),\n",
    "        \"worldcover\": os.path.join(DATA_PATH, \"worldcover\", \"Vienna_WorldCover_2021.tif\"),\n",
    "        \"geojson\": os.path.join(GEOJSON_FOLDER, \"Vienna.geojson\"),\n",
    "    },\n",
    "    \"London\": {\n",
    "        \"stack\": os.path.join(DATA_PATH, \"sentinel_stacks\", \"London_MultiMonth_stack.tif\"),\n",
    "        \"worldcover\": os.path.join(DATA_PATH, \"worldcover\", \"London_WorldCover_2021.tif\"),\n",
    "        \"geojson\": os.path.join(GEOJSON_FOLDER, \"London.geojson\"),\n",
    "    },\n",
    "    \"Melbourne\": {\n",
    "        \"stack\": os.path.join(DATA_PATH, \"sentinel_stacks\", \"Melbourne_MultiMonth_stack.tif\"),\n",
    "        \"worldcover\": os.path.join(DATA_PATH, \"worldcover\", \"Melbourne_WorldCover_2021.tif\"),\n",
    "        \"geojson\": os.path.join(GEOJSON_FOLDER, \"Melbourne.geojson\"),\n",
    "    },\n",
    "    \"Paris\": {\n",
    "        \"stack\": os.path.join(DATA_PATH, \"sentinel_stacks\", \"Paris_MultiMonth_stack.tif\"),\n",
    "        \"worldcover\": os.path.join(DATA_PATH, \"worldcover\", \"Paris_WorldCover_2021.tif\"),\n",
    "        \"geojson\": os.path.join(GEOJSON_FOLDER, \"Paris.geojson\"),\n",
    "    },\n",
    "    \"San_Francisco\": {\n",
    "        \"stack\": os.path.join(DATA_PATH, \"sentinel_stacks\", \"San_Francisco_MultiMonth_stack.tif\"),\n",
    "        \"worldcover\": os.path.join(DATA_PATH, \"worldcover\", \"San_Francisco_WorldCover_2021.tif\"),\n",
    "        \"geojson\": os.path.join(GEOJSON_FOLDER, \"San_Francisco.geojson\"),\n",
    "    },\n",
    "    \"Seattle\": {\n",
    "        \"stack\": os.path.join(DATA_PATH, \"sentinel_stacks\", \"Seattle_MultiMonth_stack.tif\"),\n",
    "        \"worldcover\": os.path.join(DATA_PATH, \"worldcover\", \"Seattle_WorldCover_2021.tif\"),\n",
    "        \"geojson\": os.path.join(GEOJSON_FOLDER, \"Seattle.geojson\"),\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"Configuration loaded\")\n",
    "print(f\"  Project root: {PROJECT_ROOT}\")\n",
    "print(f\"  Data path: {DATA_PATH}\")\n",
    "print(f\"  Models path: {MODELS_PATH}\")\n",
    "print(f\"  Output folder: {RUN_FOLDER}\")\n",
    "print(f\"  Target cities: {len(CITY_FILES)}\")\n",
    "for city in CITY_FILES:\n",
    "    print(f\"    - {city}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Discover Available Cities\n",
    "### Find all cities with Multi-Month stacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\" * 70)\nprint(\"DISCOVERING AVAILABLE CITIES\")\nprint(\"=\" * 70)\n\nprint(f\"\\nChecking {len(CITY_FILES)} configured cities...\")\n\ncities_data = []\nmissing_cities = []\n\nfor city_name, paths in CITY_FILES.items():\n    stack_file = paths[\"stack\"]\n    worldcover_file = paths[\"worldcover\"]\n    geojson_file = paths[\"geojson\"]\n    \n    has_stack = os.path.exists(stack_file)\n    has_worldcover = os.path.exists(worldcover_file)\n    has_geojson = os.path.exists(geojson_file)\n    \n    status_stack = \"Y\" if has_stack else \"N\"\n    status_geojson = \"Y\" if has_geojson else \"N\"\n    status_worldcover = \"Y\" if has_worldcover else \"N\"\n    \n    print(f\"  {city_name:15s} - Stack: {status_stack}  GeoJSON: {status_geojson}  WorldCover: {status_worldcover}\")\n    \n    if has_stack and has_geojson and has_worldcover:\n        cities_data.append({\n            \"name\": city_name,\n            \"stack_file\": stack_file,\n            \"geojson_file\": geojson_file,\n            \"worldcover_file\": worldcover_file\n        })\n    else:\n        missing = []\n        if not has_stack: missing.append(\"Stack\")\n        if not has_geojson: missing.append(\"GeoJSON\")\n        if not has_worldcover: missing.append(\"WorldCover\")\n        missing_cities.append(f\"{city_name} (missing: {', '.join(missing)})\")\n\ncomplete_cities = cities_data\n\nprint(f\"\\n{'=' * 70}\")\nprint(f\"Cities with complete data: {len(complete_cities)}/{len(CITY_FILES)}\")\nprint(f\"{'=' * 70}\")\n\nif missing_cities:\n    print(f\"\\nSkipped cities:\")\n    for city in missing_cities:\n        print(f\"  - {city}\")\n\nif len(complete_cities) == 0:\n    raise ValueError(\"No cities with complete data found! Check your file paths.\")\n\nprint(f\"\\nReady to train with {len(complete_cities)} cities:\")\nfor city in complete_cities:\n    print(f\"  - {city['name']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load and Process All Cities\n",
    "### Load Sentinel-2 stacks and create WorldCover labels for each city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\" * 70)\nprint(\"LOADING AND PROCESSING ALL CITIES\")\nprint(\"=\" * 70)\n\nEXPECTED_BANDS = None\n\n# cap per class per city to keep memory manageable and balance the dataset\nMAX_SAMPLES_PER_CLASS_PER_CITY = 100000\n\nall_X = []\nall_y = []\ncity_info = []\nskipped_cities = []\n\nfor city_data in tqdm(complete_cities, desc=\"Processing cities\"):\n    city_name = city_data[\"name\"]\n    stack_file = city_data[\"stack_file\"]\n    worldcover_file = city_data[\"worldcover_file\"]\n    \n    print(f\"\\n{'=' * 70}\")\n    print(f\"Processing: {city_name}\")\n    print(f\"{'=' * 70}\")\n    \n    try:\n        with rasterio.open(stack_file) as src:\n            X_stack = src.read()\n            stack_transform = src.transform\n            stack_shape = (src.height, src.width)\n            stack_crs = src.crs\n        \n        n_bands = X_stack.shape[0]\n        print(f\"  Loaded Sentinel-2 stack: {X_stack.shape} ({n_bands} bands)\")\n        \n        if EXPECTED_BANDS is None:\n            EXPECTED_BANDS = n_bands\n            print(f\"  Setting expected bands to {EXPECTED_BANDS}\")\n        elif n_bands != EXPECTED_BANDS:\n            print(f\"  SKIPPING: Expected {EXPECTED_BANDS} bands, but found {n_bands}\")\n            skipped_cities.append({\"name\": city_name, \"reason\": f\"Band mismatch: {n_bands} vs {EXPECTED_BANDS}\"})\n            continue\n        \n        # reproject WorldCover to match Sentinel-2 grid\n        with rasterio.open(worldcover_file) as src:\n            worldcover_data = np.empty(stack_shape, dtype=np.uint8)\n            reproject(\n                source=rasterio.band(src, 1),\n                destination=worldcover_data,\n                src_transform=src.transform,\n                src_crs=src.crs,\n                dst_transform=stack_transform,\n                dst_crs=stack_crs,\n                resampling=Resampling.nearest\n            )\n        \n        # binary green / non-green\n        labels = np.isin(worldcover_data, GREEN_CLASSES).astype(np.uint8)\n        \n        green_percentage = 100 * labels.sum() / labels.size\n        print(f\"  WorldCover labels: {labels.shape} ({green_percentage:.2f}% green)\")\n        \n        n_pixels = X_stack.shape[1] * X_stack.shape[2]\n        X = X_stack.reshape(n_bands, -1).T\n        y = labels.flatten()\n        \n        valid_mask = ~np.isnan(X).any(axis=1)\n        X_clean = X[valid_mask]\n        y_clean = y[valid_mask]\n        \n        print(f\"  Valid samples: {len(X_clean):,} ({100 * len(X_clean) / n_pixels:.1f}% of pixels)\")\n        \n        # balanced sampling: equal green and non-green\n        green_indices = np.where(y_clean == 1)[0]\n        nongreen_indices = np.where(y_clean == 0)[0]\n        \n        n_samples_per_class = min(len(green_indices), len(nongreen_indices), MAX_SAMPLES_PER_CLASS_PER_CITY)\n        \n        np.random.seed(42)\n        sampled_green = np.random.choice(green_indices, n_samples_per_class, replace=False)\n        sampled_nongreen = np.random.choice(nongreen_indices, n_samples_per_class, replace=False)\n        \n        sampled_indices = np.concatenate([sampled_green, sampled_nongreen])\n        np.random.shuffle(sampled_indices)\n        \n        X_sampled = X_clean[sampled_indices]\n        y_sampled = y_clean[sampled_indices]\n        \n        print(f\"  Balanced sampling: {len(X_sampled):,} samples ({n_samples_per_class:,} per class)\")\n        print(f\"    Green: {np.sum(y_sampled == 1):,} ({100 * np.sum(y_sampled == 1) / len(y_sampled):.1f}%)\")\n        print(f\"    Non-green: {np.sum(y_sampled == 0):,} ({100 * np.sum(y_sampled == 0) / len(y_sampled):.1f}%)\")\n        \n        all_X.append(X_sampled)\n        all_y.append(y_sampled)\n        city_info.extend([city_name] * len(X_sampled))\n        \n    except Exception as e:\n        print(f\"  Error processing {city_name}: {e}\")\n        skipped_cities.append({\"name\": city_name, \"reason\": str(e)})\n        continue\n\nprint(f\"\\n{'=' * 70}\")\nprint(\"DATA AGGREGATION\")\nprint(f\"{'=' * 70}\")\n\nif skipped_cities:\n    print(f\"\\nSkipped {len(skipped_cities)} cities:\")\n    for city in skipped_cities:\n        print(f\"  - {city['name']}: {city['reason']}\")\n\nif len(all_X) == 0:\n    raise ValueError(\"No valid city data loaded! Check that all stacks have the same number of bands.\")\n\nX_combined = np.vstack(all_X)\ny_combined = np.hstack(all_y)\ncity_info = np.array(city_info)\n\nprint(f\"\\nCombined dataset (balanced):\")\nprint(f\"  Cities included: {len(all_X)}\")\nprint(f\"  Total samples: {len(X_combined):,}\")\nprint(f\"  Features (bands): {X_combined.shape[1]}\")\nprint(f\"  Green: {np.sum(y_combined == 1):,} ({100 * np.sum(y_combined == 1) / len(y_combined):.1f}%)\")\nprint(f\"  Non-green: {np.sum(y_combined == 0):,} ({100 * np.sum(y_combined == 0) / len(y_combined):.1f}%)\")\n\nprint(f\"\\nSamples per city:\")\nfor city_name in np.unique(city_info):\n    city_samples = np.sum(city_info == city_name)\n    print(f\"  {city_name:20s}: {city_samples:>10,}\")\n\nprint(f\"{'=' * 70}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train-Test Split\n",
    "### Split data for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\" * 70)\nprint(\"TRAIN-TEST SPLIT & FEATURE NORMALIZATION\")\nprint(\"=\" * 70)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X_combined, y_combined, \n    test_size=0.2, \n    random_state=42, \n    stratify=y_combined\n)\n\nprint(f\"\\nDataset split:\")\nprint(f\"  Training samples: {len(X_train):,}\")\nprint(f\"    Green: {np.sum(y_train == 1):,} ({100 * np.sum(y_train == 1) / len(y_train):.1f}%)\")\nprint(f\"    Non-green: {np.sum(y_train == 0):,} ({100 * np.sum(y_train == 0) / len(y_train):.1f}%)\")\nprint(f\"\\n  Testing samples: {len(X_test):,}\")\nprint(f\"    Green: {np.sum(y_test == 1):,} ({100 * np.sum(y_test == 1) / len(y_test):.1f}%)\")\nprint(f\"    Non-green: {np.sum(y_test == 0):,} ({100 * np.sum(y_test == 0) / len(y_test):.1f}%)\")\n\nprint(f\"\\nApplying StandardScaler...\")\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nprint(f\"  Features normalized\")\nprint(f\"    Mean (train): {X_train_scaled.mean():.4f}\")\nprint(f\"    Std (train): {X_train_scaled.std():.4f}\")\n\nprint(f\"{'=' * 70}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train Random Forest Model\n",
    "### Train on multi-city dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\" * 70)\nprint(\"TRAINING RANDOM FOREST MODEL\")\nprint(\"=\" * 70)\n\nrf = RandomForestClassifier(\n    n_estimators=200,\n    max_depth=None,\n    min_samples_split=10,\n    min_samples_leaf=5,\n    max_features='sqrt',\n    class_weight=None,  # data already balanced via sampling\n    random_state=42,\n    n_jobs=-1,\n    verbose=1\n)\n\nprint(f\"\\nRandom Forest parameters:\")\nprint(f\"  n_estimators: {rf.n_estimators}\")\nprint(f\"  max_depth: {rf.max_depth}\")\nprint(f\"  min_samples_split: {rf.min_samples_split}\")\nprint(f\"  min_samples_leaf: {rf.min_samples_leaf}\")\nprint(f\"  max_features: {rf.max_features}\")\nprint(f\"  class_weight: {rf.class_weight}\")\n\nprint(f\"\\nTraining on {len(complete_cities)} cities, {len(X_train_scaled):,} samples...\")\n\nrf.fit(X_train_scaled, y_train)\n\nprint(f\"\\nModel trained\")\nprint(f\"{'=' * 70}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\" * 70)\nprint(\"MODEL EVALUATION\")\nprint(\"=\" * 70)\n\ny_pred = rf.predict(X_test_scaled)\n\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred, zero_division=0)\nrecall = recall_score(y_test, y_pred, zero_division=0)\nf1 = f1_score(y_test, y_pred, zero_division=0)\n\nprint(f\"\\nPerformance ({len(complete_cities)} cities):\")\nprint(f\"  Accuracy:  {accuracy:.4f}\")\nprint(f\"  Precision: {precision:.4f}\")\nprint(f\"  Recall:    {recall:.4f}\")\nprint(f\"  F1-Score:  {f1:.4f}\")\n\ncm = confusion_matrix(y_test, y_pred)\nprint(f\"\\nConfusion Matrix:\")\nprint(f\"                 Predicted\")\nprint(f\"               Non-Green  Green\")\nprint(f\"Actual Non-Green  {cm[0,0]:>8,}  {cm[0,1]:>8,}\")\nprint(f\"       Green      {cm[1,0]:>8,}  {cm[1,1]:>8,}\")\n\ntotal_actual_green = cm[1,0] + cm[1,1]\ntotal_actual_nongreen = cm[0,0] + cm[0,1]\ngreen_detected = 100 * cm[1,1] / total_actual_green if total_actual_green > 0 else 0\ngreen_missed = 100 * cm[1,0] / total_actual_green if total_actual_green > 0 else 0\n\nprint(f\"\\nGreen detection:\")\nprint(f\"  Correctly detected: {green_detected:.1f}%\")\nprint(f\"  Missed (false negative): {green_missed:.1f}%\")\n\nmetrics = {\n    \"model\": \"RandomForest_Improved\",\n    \"ground_truth\": \"WorldCover_2021\",\n    \"training_cities\": [city['name'] for city in complete_cities],\n    \"n_cities\": len(complete_cities),\n    \"total_training_samples\": int(len(X_train)),\n    \"total_testing_samples\": int(len(X_test)),\n    \"accuracy\": float(accuracy),\n    \"precision\": float(precision),\n    \"recall\": float(recall),\n    \"f1_score\": float(f1),\n    \"confusion_matrix\": cm.tolist(),\n    \"improvements\": [\n        \"Balanced sampling (equal green/non-green per city)\",\n        \"Feature normalization (StandardScaler)\",\n        \"Increased n_estimators (200)\",\n        \"Removed max_depth limit\",\n        \"Reduced min_samples_split (10)\",\n        \"Reduced min_samples_leaf (5)\",\n        \"Added class_weight='balanced'\"\n    ]\n}\n\nwith open(os.path.join(RUN_FOLDER, \"metrics.json\"), \"w\") as f:\n    json.dump(metrics, f, indent=2)\n\nprint(f\"\\nMetrics saved to: {RUN_FOLDER}/metrics.json\")\nprint(f\"{'=' * 70}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# plot confusion matrix\nplt.figure(figsize=(10, 8))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n            xticklabels=['Non-Green', 'Green'],\n            yticklabels=['Non-Green', 'Green'],\n            cbar_kws={'label': 'Count'})\nplt.title(f'Confusion Matrix - Multi-City Random Forest\\n(Trained on {len(complete_cities)} cities)', \n          fontsize=14, fontweight='bold')\nplt.ylabel('True Label', fontsize=12)\nplt.xlabel('Predicted Label', fontsize=12)\nplt.tight_layout()\nplt.savefig(os.path.join(RUN_FOLDER, 'confusion_matrix.png'), dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(\"Confusion matrix saved\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# feature importances\nimportances = rf.feature_importances_\nn_features = len(importances)\n\nif n_features == 21:\n    band_names = [\n        'B02-Apr', 'B03-Apr', 'B04-Apr', 'B08-Apr', 'NDVI-Apr', 'EVI-Apr', 'SAVI-Apr',\n        'B02-Aug', 'B03-Aug', 'B04-Aug', 'B08-Aug', 'NDVI-Aug', 'EVI-Aug', 'SAVI-Aug',\n        'B02-Nov', 'B03-Nov', 'B04-Nov', 'B08-Nov', 'NDVI-Nov', 'EVI-Nov', 'SAVI-Nov'\n    ]\nelif n_features == 12:\n    band_names = [\n        'B02-Apr', 'B03-Apr', 'B04-Apr', 'B08-Apr',\n        'B02-Aug', 'B03-Aug', 'B04-Aug', 'B08-Aug',\n        'B02-Nov', 'B03-Nov', 'B04-Nov', 'B08-Nov'\n    ]\nelse:\n    band_names = [f'Band_{i+1}' for i in range(n_features)]\n    print(f\"Using generic band names for {n_features} bands\")\n\nindices = np.argsort(importances)[::-1]\n\nplt.figure(figsize=(12, max(8, n_features * 0.4)))\nplt.barh(range(len(importances)), importances[indices], color='steelblue')\nplt.yticks(range(len(importances)), [band_names[i] for i in indices])\nplt.xlabel('Feature Importance', fontsize=12)\nplt.title(f'Random Forest Feature Importance\\n({len(all_X)} cities, {n_features} bands)', \n          fontsize=14, fontweight='bold')\nplt.grid(axis='x', alpha=0.3)\nplt.tight_layout()\nplt.savefig(os.path.join(RUN_FOLDER, 'feature_importance.png'), dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(\"Feature importance plot saved\")\nprint(f\"\\nTop 10 features:\")\nfor i in range(min(10, len(importances))):\n    idx = indices[i]\n    print(f\"  {i+1:2d}. {band_names[idx]:12s}: {importances[idx]:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import joblib\nimport shutil\n\n# save model and scaler to run folder\nmodel_file = os.path.join(RUN_FOLDER, 'random_forest_model.pkl')\njoblib.dump(rf, model_file)\n\nscaler_file = os.path.join(RUN_FOLDER, 'feature_scaler.pkl')\njoblib.dump(scaler, scaler_file)\n\n# copy to main models folder for the predictor app\nmain_model_file = os.path.join(MODELS_PATH, 'random_forest_model.pkl')\nmain_scaler_file = os.path.join(MODELS_PATH, 'feature_scaler.pkl')\nshutil.copy(model_file, main_model_file)\nshutil.copy(scaler_file, main_scaler_file)\n\nprint(f\"Model saved to: {model_file}\")\nprint(f\"Scaler saved to: {scaler_file}\")\nprint(f\"\\nCopied to models folder:\")\nprint(f\"  {main_model_file}\")\nprint(f\"  {main_scaler_file}\")\nprint(f\"\\nRemember to apply scaler.transform(X) before prediction.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Per-City Performance Analysis (Optional)\n",
    "### Evaluate model performance on each city individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\" * 70)\nprint(\"PER-CITY PERFORMANCE ANALYSIS\")\nprint(\"=\" * 70)\n\nper_city_results = []\n\nfor city_data in complete_cities:\n    city_name = city_data[\"name\"]\n    stack_file = city_data[\"stack_file\"]\n    worldcover_file = city_data[\"worldcover_file\"]\n    \n    print(f\"\\n{city_name}:\")\n    \n    try:\n        with rasterio.open(stack_file) as src:\n            X_stack = src.read()\n            stack_transform = src.transform\n            stack_shape = (src.height, src.width)\n            stack_crs = src.crs\n        \n        with rasterio.open(worldcover_file) as src:\n            worldcover_data = np.empty(stack_shape, dtype=np.uint8)\n            reproject(\n                source=rasterio.band(src, 1),\n                destination=worldcover_data,\n                src_transform=src.transform,\n                src_crs=src.crs,\n                dst_transform=stack_transform,\n                dst_crs=stack_crs,\n                resampling=Resampling.nearest\n            )\n        \n        labels = np.isin(worldcover_data, GREEN_CLASSES).astype(np.uint8)\n        \n        X = X_stack.reshape(X_stack.shape[0], -1).T\n        y = labels.flatten()\n        valid_mask = ~np.isnan(X).any(axis=1)\n        X_city = X[valid_mask]\n        y_city = y[valid_mask]\n        \n        X_city_scaled = scaler.transform(X_city)\n        y_pred_city = rf.predict(X_city_scaled)\n        \n        acc = accuracy_score(y_city, y_pred_city)\n        prec = precision_score(y_city, y_pred_city, zero_division=0)\n        rec = recall_score(y_city, y_pred_city, zero_division=0)\n        f1_city = f1_score(y_city, y_pred_city, zero_division=0)\n        \n        gt_green_pct = 100 * np.sum(y_city == 1) / len(y_city)\n        pred_green_pct = 100 * np.sum(y_pred_city == 1) / len(y_pred_city)\n        diff_pct = pred_green_pct - gt_green_pct\n        \n        print(f\"  Accuracy:  {acc:.4f}\")\n        print(f\"  Precision: {prec:.4f}\")\n        print(f\"  Recall:    {rec:.4f}\")\n        print(f\"  F1-Score:  {f1_city:.4f}\")\n        print(f\"  Green %:   GT={gt_green_pct:.1f}%  Pred={pred_green_pct:.1f}%  (diff: {diff_pct:+.1f}%)\")\n        \n        per_city_results.append({\n            \"city\": city_name,\n            \"accuracy\": float(acc),\n            \"precision\": float(prec),\n            \"recall\": float(rec),\n            \"f1_score\": float(f1_city),\n            \"gt_green_pct\": float(gt_green_pct),\n            \"pred_green_pct\": float(pred_green_pct),\n            \"diff_pct\": float(diff_pct)\n        })\n        \n    except Exception as e:\n        print(f\"  Error: {e}\")\n\nwith open(os.path.join(RUN_FOLDER, \"per_city_metrics.json\"), \"w\") as f:\n    json.dump(per_city_results, f, indent=2)\n\nprint(f\"\\n{'=' * 70}\")\nprint(f\"Per-city metrics saved\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"MULTI-CITY TRAINING - SUMMARY REPORT\")\nprint(\"=\" * 80)\n\nprint(f\"\\nGround Truth: WorldCover 2021\")\nprint(f\"Green Classes: Tree cover (10), Shrubland (20), Grassland (30), Mangroves (95)\")\n\nprint(f\"\\nImprovements applied:\")\nprint(f\"  1. Balanced sampling (equal green/non-green per city)\")\nprint(f\"  2. Feature normalization (StandardScaler)\")\nprint(f\"  3. Increased n_estimators: 100 -> 200\")\nprint(f\"  4. Removed max_depth limit: 25 -> None\")\nprint(f\"  5. Reduced min_samples_split: 50 -> 10\")\nprint(f\"  6. Reduced min_samples_leaf: 20 -> 5\")\n\nprint(f\"\\nTraining Data:\")\nprint(f\"  Cities: {len(complete_cities)}\")\nfor city in complete_cities:\n    print(f\"    - {city['name']}\")\n\nprint(f\"\\n  Training samples: {len(X_train):,}\")\nprint(f\"  Testing samples:  {len(X_test):,}\")\n\nprint(f\"\\nOverall performance:\")\nprint(f\"  Accuracy:  {accuracy:.4f}\")\nprint(f\"  Precision: {precision:.4f}\")\nprint(f\"  Recall:    {recall:.4f}\")\nprint(f\"  F1-Score:  {f1:.4f}\")\n\nif per_city_results:\n    print(f\"\\nPer-city average:\")\n    avg_acc = np.mean([r['accuracy'] for r in per_city_results])\n    avg_prec = np.mean([r['precision'] for r in per_city_results])\n    avg_rec = np.mean([r['recall'] for r in per_city_results])\n    avg_f1 = np.mean([r['f1_score'] for r in per_city_results])\n    avg_diff = np.mean([r['diff_pct'] for r in per_city_results])\n    print(f\"  Accuracy:  {avg_acc:.4f}\")\n    print(f\"  Precision: {avg_prec:.4f}\")\n    print(f\"  Recall:    {avg_rec:.4f}\")\n    print(f\"  F1-Score:  {avg_f1:.4f}\")\n    print(f\"  Avg green % difference: {avg_diff:+.2f}%\")\n\nprint(f\"\\nOutput files in: {RUN_FOLDER}\")\nprint(f\"  - metrics.json\")\nprint(f\"  - per_city_metrics.json\")\nprint(f\"  - confusion_matrix.png\")\nprint(f\"  - feature_importance.png\")\nprint(f\"  - random_forest_model.pkl\")\nprint(f\"  - feature_scaler.pkl\")\n\nprint(f\"\\nNote: always apply scaler.transform(X) before prediction.\")\n\nprint(f\"\\n\" + \"=\" * 80)\nprint(f\"TRAINING COMPLETE\")\nprint(f\"=\" * 80)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "InfraVenv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}