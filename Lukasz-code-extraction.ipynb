{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "350d5d86",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'shapely'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mshapely\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeometry\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mshapely\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transform\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyproj\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'shapely'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import shapely.geometry\n",
    "from shapely.ops import transform\n",
    "import pyproj\n",
    "\n",
    "# Make folder if it doesn't exist\n",
    "folder = \"aois_json\"\n",
    "os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "# City centers: (lon, lat)\n",
    "cities = {\n",
    "    \"Vienna\": (16.3738, 48.2082),\n",
    "    \"Paris\": (2.3522, 48.8566),\n",
    "    \"London\": (-0.1276, 51.5074),\n",
    "    \"Toronto\": (-79.3832, 43.6532),\n",
    "    \"Vancouver\": (-123.1207, 49.2827),\n",
    "    \"San Francisco\": (-122.4194, 37.7749),\n",
    "    \"Lisbon\": (-9.1393, 38.7223),\n",
    "    \"Madrid\": (-3.7038, 40.4168),\n",
    "    \"Barcelona\": (2.1734, 41.3851),\n",
    "    \"Berlin\": (13.4050, 52.5200),\n",
    "    \"Amsterdam\": (4.9041, 52.3676),\n",
    "    \"Melbourne\": (144.9631, -37.8136),\n",
    "    \"Sydney\": (151.2093, -33.8688),\n",
    "    \"Auckland\": (174.7633, -36.8485),\n",
    "    \"Seattle\": (-122.3321, 47.6062)\n",
    "}\n",
    "\n",
    "# AOI size in meters (~5.12 km to match 512x512 pixels at 10 m)\n",
    "size_m = 512 * 10  \n",
    "\n",
    "def create_square_aoi(center_lon, center_lat, size_m):\n",
    "    \"\"\"\n",
    "    Create a square AOI of size_m x size_m around the city center.\n",
    "    \"\"\"\n",
    "    utm_zone = int((center_lon + 180) / 6) + 1\n",
    "    utm = pyproj.Proj(proj='utm', zone=utm_zone, ellps='WGS84')\n",
    "    wgs84 = pyproj.Proj(proj='latlong', datum='WGS84')\n",
    "\n",
    "    project_to_utm = pyproj.Transformer.from_proj(wgs84, utm, always_xy=True).transform\n",
    "    project_to_wgs = pyproj.Transformer.from_proj(utm, wgs84, always_xy=True).transform\n",
    "\n",
    "    point = shapely.geometry.Point(center_lon, center_lat)\n",
    "    point_utm = transform(project_to_utm, point)\n",
    "\n",
    "    half_size = size_m / 2\n",
    "    square_utm = shapely.geometry.box(\n",
    "        point_utm.x - half_size,\n",
    "        point_utm.y - half_size,\n",
    "        point_utm.x + half_size,\n",
    "        point_utm.y + half_size\n",
    "    )\n",
    "\n",
    "    square_wgs = transform(project_to_wgs, square_utm)\n",
    "    return square_wgs\n",
    "\n",
    "# Generate one JSON per city\n",
    "for city, (lon, lat) in cities.items():\n",
    "    square = create_square_aoi(lon, lat, size_m)\n",
    "    polygon_coords = [list(coord) for coord in square.exterior.coords]\n",
    "    geojson = {\n",
    "        \"type\": \"FeatureCollection\",\n",
    "        \"features\": [\n",
    "            {\n",
    "                \"type\": \"Feature\",\n",
    "                \"properties\": {\"city\": city},\n",
    "                \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [polygon_coords]}\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    filepath = os.path.join(folder, f\"{city.replace(' ', '_')}.geojson\")\n",
    "    with open(filepath, \"w\") as f:\n",
    "        json.dump(geojson, f, indent=2)\n",
    "    print(f\"Saved {filepath}\")\n",
    "\n",
    "print(\"All 15 city AOIs saved as separate GeoJSON files in 'aois_json' folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a699d1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PROCESSING CITY: Amsterdam\n",
      "============================================================\n",
      "\n",
      "=== Processing Amsterdam - April ===\n",
      "WARNING: Folder not found: sentinel_data/Amsterdam/Amsterdam-April-10m\n",
      "Skipping April for Amsterdam...\n",
      "\n",
      "=== Processing Amsterdam - August ===\n",
      "WARNING: Folder not found: sentinel_data/Amsterdam/Amsterdam-August-10m\n",
      "Skipping August for Amsterdam...\n",
      "\n",
      "=== Processing Amsterdam - November ===\n",
      "WARNING: Folder not found: sentinel_data/Amsterdam/Amsterdam-November-10m\n",
      "Skipping November for Amsterdam...\n",
      "\n",
      "WARNING: No bands processed for Amsterdam, skipping stack creation\n",
      "\n",
      "============================================================\n",
      "PROCESSING CITY: Auckland\n",
      "============================================================\n",
      "\n",
      "=== Processing Auckland - April ===\n",
      "WARNING: Folder not found: sentinel_data/Auckland/Auckland-April-10m\n",
      "Skipping April for Auckland...\n",
      "\n",
      "=== Processing Auckland - August ===\n",
      "WARNING: Folder not found: sentinel_data/Auckland/Auckland-August-10m\n",
      "Skipping August for Auckland...\n",
      "\n",
      "=== Processing Auckland - November ===\n",
      "WARNING: Folder not found: sentinel_data/Auckland/Auckland-November-10m\n",
      "Skipping November for Auckland...\n",
      "\n",
      "WARNING: No bands processed for Auckland, skipping stack creation\n",
      "\n",
      "============================================================\n",
      "PROCESSING CITY: Barcelona\n",
      "============================================================\n",
      "\n",
      "=== Processing Barcelona - April ===\n",
      "WARNING: Folder not found: sentinel_data/Barcelona/Barcelona-April-10m\n",
      "Skipping April for Barcelona...\n",
      "\n",
      "=== Processing Barcelona - August ===\n",
      "WARNING: Folder not found: sentinel_data/Barcelona/Barcelona-August-10m\n",
      "Skipping August for Barcelona...\n",
      "\n",
      "=== Processing Barcelona - November ===\n",
      "WARNING: Folder not found: sentinel_data/Barcelona/Barcelona-November-10m\n",
      "Skipping November for Barcelona...\n",
      "\n",
      "WARNING: No bands processed for Barcelona, skipping stack creation\n",
      "\n",
      "============================================================\n",
      "PROCESSING CITY: Berlin\n",
      "============================================================\n",
      "\n",
      "=== Processing Berlin - April ===\n",
      "WARNING: Folder not found: sentinel_data/Berlin/Berlin-April-10m\n",
      "Skipping April for Berlin...\n",
      "\n",
      "=== Processing Berlin - August ===\n",
      "WARNING: Folder not found: sentinel_data/Berlin/Berlin-August-10m\n",
      "Skipping August for Berlin...\n",
      "\n",
      "=== Processing Berlin - November ===\n",
      "WARNING: Folder not found: sentinel_data/Berlin/Berlin-November-10m\n",
      "Skipping November for Berlin...\n",
      "\n",
      "WARNING: No bands processed for Berlin, skipping stack creation\n",
      "\n",
      "============================================================\n",
      "PROCESSING CITY: Kyiv\n",
      "============================================================\n",
      "\n",
      "=== Processing Kyiv - April ===\n",
      "WARNING: Folder not found: sentinel_data/Kyiv/Kyiv-April-10m\n",
      "Skipping April for Kyiv...\n",
      "\n",
      "=== Processing Kyiv - August ===\n",
      "WARNING: Folder not found: sentinel_data/Kyiv/Kyiv-August-10m\n",
      "Skipping August for Kyiv...\n",
      "\n",
      "=== Processing Kyiv - November ===\n",
      "WARNING: Folder not found: sentinel_data/Kyiv/Kyiv-November-10m\n",
      "Skipping November for Kyiv...\n",
      "\n",
      "WARNING: No bands processed for Kyiv, skipping stack creation\n",
      "\n",
      "============================================================\n",
      "ALL CITIES PROCESSED!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import rioxarray as rxr\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# ------------------------------\n",
    "# Paths and Configuration\n",
    "# ------------------------------\n",
    "aoi_base_folder = \"aois_json\"\n",
    "sentinel_base_folder = \"sentinel_data\"\n",
    "\n",
    "# Define cities and months\n",
    "cities = [\"Amsterdam\", \"Auckland\", \"Barcelona\", \"Berlin\", \"Kyiv\"]\n",
    "months = [\"April\", \"August\", \"November\"]\n",
    "band_substrings = [\"B02\", \"B03\", \"B04\", \"B08\"]\n",
    "\n",
    "# ------------------------------\n",
    "# Process each city\n",
    "# ------------------------------\n",
    "for city in cities:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"PROCESSING CITY: {city}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Define paths for this city\n",
    "    aoi_file = os.path.join(aoi_base_folder, f\"{city}.geojson\")\n",
    "    output_file = os.path.join(sentinel_base_folder, city, f\"{city}_MultiMonth_stack.tif\")\n",
    "    \n",
    "    # Check if AOI file exists\n",
    "    if not os.path.exists(aoi_file):\n",
    "        print(f\"WARNING: AOI file not found for {city}: {aoi_file}\")\n",
    "        print(f\"Skipping {city}...\")\n",
    "        continue\n",
    "    \n",
    "    # ------------------------------\n",
    "    # Load AOI and extract geometries\n",
    "    # ------------------------------\n",
    "    aoi = gpd.read_file(aoi_file)\n",
    "    \n",
    "    # Merge multiple features if needed\n",
    "    if len(aoi) > 1:\n",
    "        merged_geom = aoi.unary_union\n",
    "        geometries = [merged_geom]\n",
    "    else:\n",
    "        geometries = [aoi.geometry.iloc[0]]\n",
    "    \n",
    "    # Ensure geometries are in WGS84\n",
    "    for i, g in enumerate(geometries):\n",
    "        if aoi.crs is None:\n",
    "            aoi.set_crs(\"EPSG:4326\", inplace=True)\n",
    "        if aoi.crs.to_epsg() != 4326:\n",
    "            geometries[i] = g.to_crs(\"EPSG:4326\")\n",
    "    \n",
    "    # ------------------------------\n",
    "    # Process each month for this city\n",
    "    # ------------------------------\n",
    "    all_band_arrays = []\n",
    "    all_band_names = []\n",
    "    \n",
    "    for month in months:\n",
    "        folder_path = os.path.join(sentinel_base_folder, city, f\"{city}-{month}-10m\")\n",
    "        print(f\"\\n=== Processing {city} - {month} ===\")\n",
    "        \n",
    "        # Check if folder exists\n",
    "        if not os.path.exists(folder_path):\n",
    "            print(f\"WARNING: Folder not found: {folder_path}\")\n",
    "            print(f\"Skipping {month} for {city}...\")\n",
    "            continue\n",
    "        \n",
    "        month_band_dict = {}\n",
    "        \n",
    "        # Load and clip each band for this month\n",
    "        for substring in band_substrings:\n",
    "            matched_files = glob.glob(os.path.join(folder_path, f\"*{substring}*\"))\n",
    "            if not matched_files:\n",
    "                print(f\"WARNING: No file found for band '{substring}' in {folder_path}\")\n",
    "                continue\n",
    "            \n",
    "            band_path = matched_files[0]\n",
    "            band = rxr.open_rasterio(band_path, masked=True).squeeze()\n",
    "            \n",
    "            # Clip to AOI using plain list of shapely geometries\n",
    "            band_clipped = band.rio.clip(geometries, crs=\"EPSG:4326\")\n",
    "            \n",
    "            # Store with month-specific name\n",
    "            band_name = f\"{substring}-{month}\"\n",
    "            all_band_arrays.append(band_clipped)\n",
    "            all_band_names.append(band_name)\n",
    "            month_band_dict[substring] = band_clipped\n",
    "            \n",
    "            print(f\"Loaded and clipped {band_name} -> shape: {band_clipped.shape}\")\n",
    "        \n",
    "        # Only calculate indices if all required bands are present\n",
    "        if len(month_band_dict) < 3:\n",
    "            print(f\"WARNING: Missing bands for {month}, skipping index calculations\")\n",
    "            continue\n",
    "        \n",
    "        # ------------------------------\n",
    "        # Calculate NDVI for this month\n",
    "        # ------------------------------\n",
    "        if \"B08\" in month_band_dict and \"B04\" in month_band_dict:\n",
    "            nir = month_band_dict[\"B08\"].astype(np.float32)\n",
    "            red = month_band_dict[\"B04\"].astype(np.float32)\n",
    "            \n",
    "            ndvi = (nir - red) / (nir + red)\n",
    "            ndvi = xr.where(np.isfinite(ndvi), ndvi, np.nan)\n",
    "            ndvi_name = f\"NDVI-{month}\"\n",
    "            all_band_arrays.append(ndvi)\n",
    "            all_band_names.append(ndvi_name)\n",
    "            print(f\"Calculated {ndvi_name} -> range: [{float(ndvi.min()):.3f}, {float(ndvi.max()):.3f}]\")\n",
    "        \n",
    "        # ------------------------------\n",
    "        # Calculate EVI\n",
    "        # ------------------------------\n",
    "        if \"B08\" in month_band_dict and \"B04\" in month_band_dict and \"B02\" in month_band_dict:\n",
    "            blue = month_band_dict[\"B02\"].astype(np.float32)\n",
    "            \n",
    "            evi = 2.5 * (nir - red) / (nir + 6*red - 7.5*blue + 1)\n",
    "            evi = xr.where(np.isfinite(evi), evi, np.nan)\n",
    "            evi_name = f\"EVI-{month}\"\n",
    "            all_band_arrays.append(evi)\n",
    "            all_band_names.append(evi_name)\n",
    "            print(f\"Calculated {evi_name} -> range: [{float(evi.min()):.3f}, {float(evi.max()):.3f}]\")\n",
    "        \n",
    "        # ------------------------------\n",
    "        # Calculate SAVI\n",
    "        # ------------------------------\n",
    "        if \"B08\" in month_band_dict and \"B04\" in month_band_dict:\n",
    "            L = 0.5\n",
    "            savi = ((nir - red) * (1 + L)) / (nir + red + L)\n",
    "            savi = xr.where(np.isfinite(savi), savi, np.nan)\n",
    "            savi_name = f\"SAVI-{month}\"\n",
    "            all_band_arrays.append(savi)\n",
    "            all_band_names.append(savi_name)\n",
    "            print(f\"Calculated {savi_name} -> range: [{float(savi.min()):.3f}, {float(savi.max()):.3f}]\")\n",
    "    \n",
    "    # ------------------------------\n",
    "    # Stack all bands from all months for this city\n",
    "    # ------------------------------\n",
    "    if not all_band_arrays:\n",
    "        print(f\"\\nWARNING: No bands processed for {city}, skipping stack creation\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n=== Creating final stack for {city} ===\")\n",
    "    stack = xr.concat(all_band_arrays, dim=\"band\")\n",
    "    stack = stack.assign_coords(band=all_band_names)\n",
    "    \n",
    "    # Convert entire stack to float32\n",
    "    stack = stack.astype(np.float32)\n",
    "    \n",
    "    print(f\"Stacked all bands > shape: {stack.shape}\")\n",
    "    print(f\"Total bands: {len(all_band_names)}\")\n",
    "    \n",
    "    # ------------------------------\n",
    "    # Save as GeoTIFF\n",
    "    # ------------------------------\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "    stack.rio.to_raster(output_file, dtype=np.float32)\n",
    "    print(f\"\\nSaved stacked GeoTIFF: {output_file}\")\n",
    "    print(f\"Band order: {all_band_names}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ALL CITIES PROCESSED!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dae1175",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/timgotschim/Documents/LLM/infrared.city/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PROCESSING: Amsterdam\n",
      "============================================================\n",
      "Loaded AOI for Amsterdam\n",
      "Fetching OSM green areas...\n",
      "✓ Saved: sentinel_data/Amsterdam/Amsterdam_OSM_green.geojson\n",
      "  Features: 4399\n",
      "  Total area: 16.04 km²\n",
      "\n",
      "============================================================\n",
      "PROCESSING: Auckland\n",
      "============================================================\n",
      "Loaded AOI for Auckland\n",
      "Fetching OSM green areas...\n",
      "✓ Saved: sentinel_data/Auckland/Auckland_OSM_green.geojson\n",
      "  Features: 1615\n",
      "  Total area: 5.75 km²\n",
      "\n",
      "============================================================\n",
      "PROCESSING: Barcelona\n",
      "============================================================\n",
      "Loaded AOI for Barcelona\n",
      "Fetching OSM green areas...\n",
      "✓ Saved: sentinel_data/Barcelona/Barcelona_OSM_green.geojson\n",
      "  Features: 1915\n",
      "  Total area: 6.38 km²\n",
      "\n",
      "============================================================\n",
      "PROCESSING: Berlin\n",
      "============================================================\n",
      "Loaded AOI for Berlin\n",
      "Fetching OSM green areas...\n",
      "✓ Saved: sentinel_data/Berlin/Berlin_OSM_green.geojson\n",
      "  Features: 3371\n",
      "  Total area: 20.91 km²\n",
      "\n",
      "============================================================\n",
      "PROCESSING: Kyiv\n",
      "============================================================\n",
      "Loaded AOI for Kyiv\n",
      "Fetching OSM green areas...\n",
      "✓ Saved: sentinel_data/Kyiv/Kyiv_OSM_green.geojson\n",
      "  Features: 1471\n",
      "  Total area: 24.64 km²\n",
      "\n",
      "============================================================\n",
      "ALL CITIES PROCESSED!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import osmnx as ox\n",
    "import geopandas as gpd\n",
    "import os\n",
    "\n",
    "# ------------------------------\n",
    "# Configuration\n",
    "# ------------------------------\n",
    "aoi_base_folder = \"aois_json\"\n",
    "output_base_folder = \"sentinel_data\"\n",
    "cities = [\"Amsterdam\", \"Auckland\", \"Barcelona\", \"Berlin\", \"Kyiv\"]\n",
    "\n",
    "# Define tags for green areas\n",
    "tags = {\n",
    "    \"leisure\": [\"park\", \"garden\"],\n",
    "    \"landuse\": [\"forest\", \"grass\", \"meadow\", \"village_green\"],\n",
    "    \"natural\": [\"wood\", \"scrub\"]\n",
    "}\n",
    "\n",
    "# ------------------------------\n",
    "# Process each city\n",
    "# ------------------------------\n",
    "for city in cities:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"PROCESSING: {city}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Define paths\n",
    "    aoi_file = os.path.join(aoi_base_folder, f\"{city}.geojson\")\n",
    "    output_file = os.path.join(output_base_folder, city, f\"{city}_OSM_green.geojson\")\n",
    "    \n",
    "    # Check if AOI file exists\n",
    "    if not os.path.exists(aoi_file):\n",
    "        print(f\"WARNING: AOI file not found: {aoi_file}\")\n",
    "        print(f\"Skipping {city}...\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        # Load AOI\n",
    "        aoi = gpd.read_file(aoi_file)\n",
    "        aoi = aoi.to_crs(\"EPSG:4326\")  # ensure WGS84\n",
    "        polygon = aoi.geometry.iloc[0]  # get shapely polygon\n",
    "        \n",
    "        print(f\"Loaded AOI for {city}\")\n",
    "        \n",
    "        # Fetch green features from OSM\n",
    "        print(f\"Fetching OSM green areas...\")\n",
    "        green_features = ox.features_from_polygon(polygon, tags)\n",
    "        \n",
    "        # Keep only polygons\n",
    "        green_features = green_features[green_features.geometry.type.isin([\"Polygon\", \"MultiPolygon\"])]\n",
    "        \n",
    "        if len(green_features) == 0:\n",
    "            print(f\"WARNING: No green area polygons found for {city}\")\n",
    "            continue\n",
    "        \n",
    "        # Calculate total area\n",
    "        total_area_km2 = green_features.to_crs('EPSG:3857').area.sum() / 1e6\n",
    "        \n",
    "        # Save to GeoJSON\n",
    "        os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "        green_features.to_file(output_file, driver=\"GeoJSON\")\n",
    "        \n",
    "        print(f\"✓ Saved: {output_file}\")\n",
    "        print(f\"  Features: {len(green_features)}\")\n",
    "        print(f\"  Total area: {total_area_km2:.2f} km²\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR processing {city}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ALL CITIES PROCESSED!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a181ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PROCESSING: Amsterdam\n",
      "============================================================\n",
      "WARNING: Stack file not found: sentinel_data/Amsterdam/Amsterdam_MultiMonth_stack.tif\n",
      "Skipping Amsterdam...\n",
      "\n",
      "============================================================\n",
      "PROCESSING: Auckland\n",
      "============================================================\n",
      "WARNING: Stack file not found: sentinel_data/Auckland/Auckland_MultiMonth_stack.tif\n",
      "Skipping Auckland...\n",
      "\n",
      "============================================================\n",
      "PROCESSING: Barcelona\n",
      "============================================================\n",
      "WARNING: Stack file not found: sentinel_data/Barcelona/Barcelona_MultiMonth_stack.tif\n",
      "Skipping Barcelona...\n",
      "\n",
      "============================================================\n",
      "PROCESSING: Berlin\n",
      "============================================================\n",
      "WARNING: Stack file not found: sentinel_data/Berlin/Berlin_MultiMonth_stack.tif\n",
      "Skipping Berlin...\n",
      "\n",
      "============================================================\n",
      "PROCESSING: Kyiv\n",
      "============================================================\n",
      "WARNING: Stack file not found: sentinel_data/Kyiv/Kyiv_MultiMonth_stack.tif\n",
      "Skipping Kyiv...\n",
      "\n",
      "============================================================\n",
      "ALL CITIES PROCESSED!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import rasterio\n",
    "from rasterio.features import rasterize\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# ------------------------------\n",
    "# Configuration\n",
    "# ------------------------------\n",
    "sentinel_base_folder = \"sentinel_data\"\n",
    "cities = [\"Amsterdam\", \"Auckland\", \"Barcelona\", \"Berlin\", \"Kyiv\"]\n",
    "\n",
    "# ------------------------------\n",
    "# Process each city\n",
    "# ------------------------------\n",
    "for city in cities:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"PROCESSING: {city}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Define paths\n",
    "    stack_path = os.path.join(sentinel_base_folder, city, f\"{city}_MultiMonth_stack.tif\")\n",
    "    osm_path = os.path.join(sentinel_base_folder, city, f\"{city}_OSM_green.geojson\")\n",
    "    label_path = os.path.join(sentinel_base_folder, city, f\"{city}_OSM_labels.tif\")\n",
    "    \n",
    "    # Check if required files exist\n",
    "    if not os.path.exists(stack_path):\n",
    "        print(f\"WARNING: Stack file not found: {stack_path}\")\n",
    "        print(f\"Skipping {city}...\")\n",
    "        continue\n",
    "    \n",
    "    if not os.path.exists(osm_path):\n",
    "        print(f\"WARNING: OSM green file not found: {osm_path}\")\n",
    "        print(f\"Skipping {city}...\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        # Load Sentinel-2 stack metadata\n",
    "        with rasterio.open(stack_path) as src:\n",
    "            transform = src.transform\n",
    "            out_shape = (src.height, src.width)\n",
    "            crs = src.crs\n",
    "        \n",
    "        print(f\"Loaded stack metadata: {out_shape[0]}x{out_shape[1]} pixels\")\n",
    "        \n",
    "        # Load GeoJSON of green areas from OSM\n",
    "        green_features = gpd.read_file(osm_path)\n",
    "        \n",
    "        # Ensure CRS matches Sentinel-2\n",
    "        green_features = green_features.to_crs(crs)\n",
    "        \n",
    "        # Keep only polygons\n",
    "        green_features = green_features[green_features.geometry.type.isin([\"Polygon\", \"MultiPolygon\"])]\n",
    "        \n",
    "        if len(green_features) == 0:\n",
    "            print(f\"WARNING: No polygon features found for {city}\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"Loaded {len(green_features)} green area polygons\")\n",
    "        \n",
    "        # ------------------------------\n",
    "        # Rasterize with all_touched approach\n",
    "        # ------------------------------\n",
    "        print(\"Rasterizing green areas...\")\n",
    "        \n",
    "        labels = rasterize(\n",
    "            [(geom, 1) for geom in green_features.geometry],\n",
    "            out_shape=out_shape,\n",
    "            transform=transform,\n",
    "            fill=0,\n",
    "            all_touched=True,   # include pixels partially covered\n",
    "            dtype=\"uint8\"\n",
    "        )\n",
    "        \n",
    "        green_count = np.sum(labels == 1)\n",
    "        total_pixels = labels.size\n",
    "        print(f\"  Completed: {green_count}/{total_pixels} pixels labeled as green ({100*green_count/total_pixels:.2f}%)\")\n",
    "        \n",
    "        # ------------------------------\n",
    "        # Save raster labels\n",
    "        # ------------------------------\n",
    "        with rasterio.open(\n",
    "            label_path,\n",
    "            \"w\",\n",
    "            driver=\"GTiff\",\n",
    "            height=out_shape[0],\n",
    "            width=out_shape[1],\n",
    "            count=1,\n",
    "            dtype=\"uint8\",\n",
    "            crs=crs,\n",
    "            transform=transform,\n",
    "            compress=\"lzw\"  # Add compression to reduce file size\n",
    "        ) as dst:\n",
    "            dst.write(labels, 1)\n",
    "        \n",
    "        print(f\"✓ Saved: {label_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR processing {city}: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ALL CITIES PROCESSED!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a305689b",
   "metadata": {},
   "outputs": [
    {
     "ename": "RasterioIOError",
     "evalue": "sentinel_data/Amsterdam/Amsterdam_MultiMonth_stack.tif: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCPLE_OpenFailedError\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[0;32mrasterio/_base.pyx:310\u001b[0m, in \u001b[0;36mrasterio._base.DatasetBase.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mrasterio/_base.pyx:221\u001b[0m, in \u001b[0;36mrasterio._base.open_dataset\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mrasterio/_err.pyx:359\u001b[0m, in \u001b[0;36mrasterio._err.exc_wrap_pointer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mCPLE_OpenFailedError\u001b[0m: sentinel_data/Amsterdam/Amsterdam_MultiMonth_stack.tif: No such file or directory",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRasterioIOError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Load Sentinel-2 stack\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mrasterio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msentinel_data/Amsterdam/Amsterdam_MultiMonth_stack.tif\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m src:\n\u001b[1;32m     11\u001b[0m     X \u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39mread()  \u001b[38;5;66;03m# shape: (bands, height, width)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Load labels\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/LLM/infrared.city/.venv/lib/python3.9/site-packages/rasterio/env.py:463\u001b[0m, in \u001b[0;36mensure_env_with_credentials.<locals>.wrapper\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    460\u001b[0m     session \u001b[38;5;241m=\u001b[39m DummySession()\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m env_ctor(session\u001b[38;5;241m=\u001b[39msession):\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/LLM/infrared.city/.venv/lib/python3.9/site-packages/rasterio/__init__.py:356\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, driver, width, height, count, crs, transform, dtype, nodata, sharing, opener, **kwargs)\u001b[0m\n\u001b[1;32m    353\u001b[0m     path \u001b[38;5;241m=\u001b[39m _parse_path(raw_dataset_path)\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 356\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mDatasetReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdriver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msharing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    358\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m get_writer_for_path(path, driver\u001b[38;5;241m=\u001b[39mdriver)(\n\u001b[1;32m    359\u001b[0m         path, mode, driver\u001b[38;5;241m=\u001b[39mdriver, sharing\u001b[38;5;241m=\u001b[39msharing, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    360\u001b[0m     )\n",
      "File \u001b[0;32mrasterio/_base.pyx:312\u001b[0m, in \u001b[0;36mrasterio._base.DatasetBase.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mRasterioIOError\u001b[0m: sentinel_data/Amsterdam/Amsterdam_MultiMonth_stack.tif: No such file or directory"
     ]
    }
   ],
   "source": [
    "import rasterio\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, balanced_accuracy_score, f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load Sentinel-2 stack\n",
    "with rasterio.open(\"sentinel_data/Amsterdam/Amsterdam_MultiMonth_stack.tif\") as src:\n",
    "    X = src.read()  # shape: (bands, height, width)\n",
    "\n",
    "# Load labels\n",
    "with rasterio.open(\"sentinel_data/Amsterdam/Amsterdam_OSM_labels.tif\") as src:\n",
    "    y = src.read(1)  # shape: (height, width)\n",
    "\n",
    "# Flatten to (n_samples, n_features)\n",
    "n_bands, h, w = X.shape\n",
    "X_flat = X.reshape(n_bands, -1).T  # shape: (h*w, n_bands)\n",
    "y_flat = y.flatten()                # shape: (h*w,)\n",
    "\n",
    "# Remove NaN values\n",
    "mask = ~np.isnan(X_flat).any(axis=1)\n",
    "X_flat = X_flat[mask]\n",
    "y_flat = y_flat[mask]\n",
    "\n",
    "# ------------------------------\n",
    "# Check class distribution\n",
    "# ------------------------------\n",
    "unique, counts = np.unique(y_flat, return_counts=True)\n",
    "total = len(y_flat)\n",
    "print(\"=\"*60)\n",
    "print(\"CLASS DISTRIBUTION:\")\n",
    "print(\"=\"*60)\n",
    "for label, count in zip(unique, counts):\n",
    "    print(f\"Class {label}: {count:,} samples ({100*count/total:.2f}%)\")\n",
    "print(f\"\\nImbalance ratio: {counts[0]/counts[1]:.1f}:1 (non-green:green)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ------------------------------\n",
    "# Split with stratification\n",
    "# ------------------------------\n",
    "# Stratify ensures both train and test have similar class distributions\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_flat, y_flat, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=y_flat  # Important for imbalanced data!\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain set: {len(y_train):,} samples\")\n",
    "print(f\"  Green: {np.sum(y_train==1):,} ({100*np.mean(y_train==1):.2f}%)\")\n",
    "print(f\"Test set: {len(y_test):,} samples\")\n",
    "print(f\"  Green: {np.sum(y_test==1):,} ({100*np.mean(y_test==1):.2f}%)\")\n",
    "\n",
    "# ------------------------------\n",
    "# Train Random Forest\n",
    "# ------------------------------\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING RANDOM FOREST...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "clf = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=25,\n",
    "    min_samples_split=10,      # Prevent overfitting on minority class\n",
    "    min_samples_leaf=5,        # Ensure leaves have enough samples\n",
    "    class_weight=\"balanced\",   # Handle imbalance\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# ------------------------------\n",
    "# Evaluate with multiple metrics\n",
    "# ------------------------------\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATION METRICS:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Classification report (precision, recall, F1 for each class)\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=[\"Non-green\", \"Green\"]))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(\"                Predicted\")\n",
    "print(\"              Non-green  Green\")\n",
    "print(f\"Actual Non-g  {cm[0,0]:8d}  {cm[0,1]:6d}\")\n",
    "print(f\"       Green  {cm[1,0]:8d}  {cm[1,1]:6d}\")\n",
    "\n",
    "# Additional metrics for imbalanced data\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"IMBALANCED DATA METRICS:\")\n",
    "print(\"-\"*60)\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred):.3f}\")\n",
    "print(f\"F1 Score (Green):  {f1_score(y_test, y_pred, pos_label=1):.3f}\")\n",
    "print(f\"F1 Score (Macro):  {f1_score(y_test, y_pred, average='macro'):.3f}\")\n",
    "\n",
    "# Calculate specificity and sensitivity manually\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "sensitivity = tp / (tp + fn)  # Recall for green class\n",
    "specificity = tn / (tn + fp)  # Recall for non-green class\n",
    "print(f\"Sensitivity (Green Recall):     {sensitivity:.3f}\")\n",
    "print(f\"Specificity (Non-green Recall): {specificity:.3f}\")\n",
    "\n",
    "# ------------------------------\n",
    "# Feature importance\n",
    "# ------------------------------\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BAND IMPORTANCE:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get band names (you can customize this based on your actual bands)\n",
    "band_names = [f\"Band_{i+1}\" for i in range(n_bands)]\n",
    "\n",
    "# Sort by importance\n",
    "importance_idx = np.argsort(clf.feature_importances_)[::-1]\n",
    "for idx in importance_idx[:10]:  # Top 10\n",
    "    print(f\"{band_names[idx]:15s}: {clf.feature_importances_[idx]:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
